{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imread` from `matplotlib.pyplot`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://users.aalto.fi/mvsjober/misc/dl2018-image-proj.zip to train/dl2018-image-proj.zip\n"
     ]
    }
   ],
   "source": [
    "train_path = 'train'\n",
    "dl_file = 'dl2018-image-proj.zip'\n",
    "dl_url = 'https://users.aalto.fi/mvsjober/misc/'\n",
    "\n",
    "zip_path = os.path.join(train_path, dl_file)\n",
    "if not os.path.isfile(zip_path):\n",
    "    download_url(dl_url + dl_file, root=train_path, filename=dl_file, md5=None)\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as zip_f:\n",
    "    zip_f.extractall(train_path)\n",
    "    #os.unlink(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU!')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "img_path = \"./train/images/\"\n",
    "labels_path = \"./train/annotations/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all annotations & images paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_paths = sorted([img_path + current_img_name  for current_img_name  in listdir(img_path) ])\n",
    "all_labels_paths = sorted([labels_path + current_label_name  for current_label_name  in listdir(labels_path) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_nb = [all_img_paths[i].split(\"/\")[-1].split(\".\")[0].split(\"im\")[-1] for i in range(len(all_img_paths))]\n",
    "all_labels_names = [all_labels_paths[i].split(\"/\")[-1].split(\".\")[0] for i in range(len(all_labels_paths))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABELS PREPROCESSING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD labelled Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_labels = {}\n",
    "for current_label in range(len(all_labels_paths)):\n",
    "    current_label_contents = pd.read_csv(all_labels_paths[current_label])\n",
    "    current_label_img_nb = sorted(np.append(list(current_label_contents), current_label_contents.get_values() ).tolist())\n",
    "    for current_img in current_label_img_nb:\n",
    "        if( current_img not in img_labels.keys()):\n",
    "            img_labels[current_img] = np.zeros(len(all_labels_names))\n",
    "        img_labels[current_img][current_label] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD unlabelled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmatched_set(a, b):\n",
    "    return [[a_i for a_i in a if a_i not in b], [b_i for b_i in b if b_i not in a]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get disjoined sets of unlabelled images and inexistent labelled images ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unmatched_set_images = unmatched_set( list(img_labels.keys()) , np.array(all_img_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_unlab_img in unmatched_set_images[1]:\n",
    "    img_labels[current_unlab_img] = np.zeros(len(all_labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMAGES PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 kinds of pictures :  \n",
    "  - the first kind : The shapes equal to (128, 128, 3)\n",
    "  - the second kind : The shapes equal to (128, 128)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = {}\n",
    "for current_img_path in all_img_paths:\n",
    "    current_img_vector =  plt.imread( current_img_path )\n",
    "    if(str(current_img_vector.shape) not in img_shape.keys()):\n",
    "        img_shape[str(current_img_vector.shape)] = []\n",
    "    img_shape[str(current_img_vector.shape)].append(current_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['(128, 128, 3)', '(128, 128)'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vector_3 = {}\n",
    "#To process with all images, don't forget to remove \"[:10]\"\n",
    "for current_img_path in img_shape[\"(128, 128, 3)\"][:1024]:\n",
    "    I =  plt.imread( current_img_path )\n",
    "    modified_current_img_vector = np.array([I[:,:,0], I[:,:,1], I[:,:,2]])\n",
    "    img_vector_3[current_img_path.split(\"/\")[-1].split(\".\")[0].split(\"im\")[-1]] = modified_current_img_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grey images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vector_1 = {}\n",
    "\n",
    "for current_img_path in img_shape[\"(128, 128)\"]:\n",
    "    current_img_vector =  plt.imread( current_img_path )\n",
    "    #Do we have to reshape to (128*128) or (1,128*128) ?\n",
    "    img_vector_1[current_img_path.split(\"/\")[-1].split(\".\")[0].split(\"im\")[-1]] = np.reshape(current_img_vector, (1,128,128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_img_vector_3 = len(img_vector_3.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train dataset and test dataset from 3 dimention images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "img_names = rd.sample(list(img_vector_3.keys()), nb_img_vector_3)\n",
    "train_img_name = img_names[ :int(nb_img_vector_3 * 0.8)]\n",
    "test_img_name = img_names[ int(nb_img_vector_3 * 0.8):]\n",
    "\n",
    "\n",
    "X_Train = torch.from_numpy(np.array([ [ img_vector_3[ train_img_name[id_name*batch_size + id_batch]]  for id_batch in range(batch_size) ] for id_name in range(int(len(train_img_name)/batch_size))]))\n",
    "y_Train = torch.from_numpy(np.array([ [ img_labels[   train_img_name[id_name*batch_size + id_batch]]  for id_batch in range(batch_size) ]for id_name in range(int(len(train_img_name)/batch_size))]))\n",
    "\n",
    "X_Test = torch.from_numpy(np.array([ [ img_vector_3[ test_img_name[id_name*batch_size + id_batch]]  for id_batch in range(batch_size) ] for id_name in range(int(len(test_img_name)/batch_size))]))\n",
    "y_Test = torch.from_numpy(np.array([ [ img_labels[   test_img_name[id_name*batch_size + id_batch]]  for id_batch in range(batch_size) ]for id_name in range(int(len(test_img_name)/batch_size))]))\n",
    "\n",
    "X_Train = X_Train.type('torch.FloatTensor')\n",
    "y_Train = y_Train.type('torch.FloatTensor')\n",
    "X_Test = X_Test.type('torch.FloatTensor')\n",
    "y_Test = y_Test.type('torch.FloatTensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64, 3, 128, 128])\n",
      "torch.Size([12, 64, 14])\n",
      "torch.Size([3, 64, 3, 128, 128])\n",
      "torch.Size([3, 64, 14])\n"
     ]
    }
   ],
   "source": [
    "print(X_Train.shape)\n",
    "print(y_Train.shape)\n",
    "print(X_Test.shape)\n",
    "print(y_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 3, 128, 128])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 128, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = zip(X_Train, y_Train)\n",
    "test_loader = zip(X_Test, y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "0\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "1\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "2\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "3\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "4\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "5\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "6\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "7\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "8\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "9\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64, 14])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for  batch_idx, (data, target) in enumerate(train_loader) :\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    print(batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (do): Dropout(p=0.5)\n",
      "  (bn1): BatchNorm1d(18000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=18000, out_features=240, bias=True)\n",
      "  (fc2): Linear(in_features=240, out_features=120, bias=True)\n",
      "  (fcout): Linear(in_features=120, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.do = nn.Dropout(p=0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=18000)\n",
    "        self.fc1 = nn.Linear(18000, 240)\n",
    "        self.fc2 = nn.Linear(240, 120)\n",
    "        self.fcout = nn.Linear(120, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.do(x)\n",
    "        x =  F.relu(self.bn1(x))\n",
    "        x = F.relu((self.fc1(x)))\n",
    "        x = F.relu((self.fc2(x)))\n",
    "        x = F.log_softmax(self.fcout(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = Net( )#.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6899, -2.7569, -2.6692, -2.6296, -2.5993, -2.6312, -2.7027, -2.7495,\n",
       "         -2.5297, -2.7054, -2.6273, -2.6172, -2.5819, -2.4954],\n",
       "        [-2.6685, -2.7440, -2.6553, -2.5907, -2.5834, -2.6164, -2.6697, -2.7932,\n",
       "         -2.5452, -2.7378, -2.7034, -2.6131, -2.5453, -2.5245],\n",
       "        [-2.6803, -2.7571, -2.6869, -2.6151, -2.5898, -2.6008, -2.6823, -2.6790,\n",
       "         -2.5968, -2.7356, -2.6304, -2.6273, -2.5673, -2.5254],\n",
       "        [-2.6780, -2.7354, -2.6554, -2.6097, -2.5995, -2.6485, -2.7248, -2.7138,\n",
       "         -2.5579, -2.7064, -2.6376, -2.6320, -2.5729, -2.5052],\n",
       "        [-2.6646, -2.8393, -2.6370, -2.5931, -2.5790, -2.5678, -2.6486, -2.7410,\n",
       "         -2.5439, -2.7338, -2.6833, -2.6081, -2.6648, -2.4941],\n",
       "        [-2.6623, -2.7782, -2.6019, -2.5803, -2.6188, -2.6427, -2.7015, -2.7456,\n",
       "         -2.5582, -2.7160, -2.7286, -2.5846, -2.5391, -2.5313],\n",
       "        [-2.6920, -2.7627, -2.6730, -2.5706, -2.6032, -2.6387, -2.6815, -2.7004,\n",
       "         -2.5685, -2.7386, -2.6336, -2.6042, -2.5797, -2.5310],\n",
       "        [-2.6650, -2.7247, -2.6629, -2.5397, -2.6591, -2.6535, -2.6440, -2.7698,\n",
       "         -2.5272, -2.7948, -2.6785, -2.6035, -2.5469, -2.5255],\n",
       "        [-2.6675, -2.7653, -2.5949, -2.5160, -2.5968, -2.6027, -2.6363, -2.7598,\n",
       "         -2.5907, -2.7971, -2.7083, -2.5859, -2.6172, -2.5546],\n",
       "        [-2.6275, -2.7740, -2.6288, -2.5309, -2.6381, -2.6489, -2.6795, -2.7284,\n",
       "         -2.5758, -2.7671, -2.6399, -2.6225, -2.5758, -2.5457],\n",
       "        [-2.6871, -2.7301, -2.6477, -2.6122, -2.5888, -2.6249, -2.6304, -2.7664,\n",
       "         -2.5496, -2.7291, -2.6624, -2.6073, -2.6248, -2.5174],\n",
       "        [-2.6723, -2.7252, -2.5782, -2.6055, -2.6126, -2.6461, -2.6798, -2.7658,\n",
       "         -2.5723, -2.7105, -2.6350, -2.6031, -2.6227, -2.5435],\n",
       "        [-2.6555, -2.7550, -2.6576, -2.5628, -2.6356, -2.6470, -2.7075, -2.6441,\n",
       "         -2.5780, -2.7484, -2.6383, -2.5679, -2.6181, -2.5570],\n",
       "        [-2.6442, -2.7745, -2.5705, -2.5725, -2.6295, -2.6750, -2.6800, -2.8096,\n",
       "         -2.5321, -2.7251, -2.6537, -2.6054, -2.5643, -2.5556],\n",
       "        [-2.6830, -2.7683, -2.6680, -2.5977, -2.6241, -2.6078, -2.6729, -2.6908,\n",
       "         -2.5519, -2.6900, -2.6171, -2.6130, -2.6381, -2.5470],\n",
       "        [-2.7280, -2.7721, -2.6185, -2.5679, -2.6227, -2.6644, -2.5334, -2.7809,\n",
       "         -2.5013, -2.7715, -2.6348, -2.6324, -2.6651, -2.5120],\n",
       "        [-2.6282, -2.7678, -2.6691, -2.5840, -2.5745, -2.6090, -2.6885, -2.6945,\n",
       "         -2.6047, -2.7587, -2.6603, -2.5936, -2.5851, -2.5579],\n",
       "        [-2.6715, -2.7376, -2.6628, -2.5868, -2.5858, -2.5867, -2.6735, -2.7411,\n",
       "         -2.5404, -2.7730, -2.6725, -2.6251, -2.5699, -2.5557],\n",
       "        [-2.6601, -2.7787, -2.6527, -2.5689, -2.5994, -2.6596, -2.6953, -2.7230,\n",
       "         -2.5421, -2.7179, -2.6775, -2.5957, -2.5675, -2.5433],\n",
       "        [-2.7139, -2.7862, -2.6976, -2.5219, -2.5608, -2.6213, -2.6879, -2.6824,\n",
       "         -2.5998, -2.7695, -2.6211, -2.5723, -2.5841, -2.5705],\n",
       "        [-2.6965, -2.7394, -2.6879, -2.5702, -2.6028, -2.6302, -2.6892, -2.6809,\n",
       "         -2.5747, -2.7446, -2.6300, -2.5952, -2.5871, -2.5453],\n",
       "        [-2.6606, -2.7219, -2.6842, -2.5865, -2.6049, -2.6204, -2.6657, -2.7167,\n",
       "         -2.5850, -2.7489, -2.6381, -2.6301, -2.5615, -2.5472],\n",
       "        [-2.6440, -2.7554, -2.6399, -2.5858, -2.6052, -2.6301, -2.7056, -2.7232,\n",
       "         -2.5344, -2.7564, -2.6665, -2.5899, -2.6041, -2.5400],\n",
       "        [-2.6399, -2.7423, -2.6268, -2.5252, -2.6337, -2.6519, -2.6670, -2.7770,\n",
       "         -2.5954, -2.7270, -2.6203, -2.6186, -2.6421, -2.5144],\n",
       "        [-2.7220, -2.7309, -2.6122, -2.5461, -2.6432, -2.7101, -2.6853, -2.7613,\n",
       "         -2.5174, -2.7009, -2.7273, -2.5751, -2.5667, -2.4995],\n",
       "        [-2.6606, -2.7647, -2.6671, -2.5942, -2.5987, -2.6604, -2.6398, -2.7132,\n",
       "         -2.5706, -2.7499, -2.6515, -2.5611, -2.6137, -2.5320],\n",
       "        [-2.6610, -2.7642, -2.6134, -2.5921, -2.6174, -2.6561, -2.6217, -2.7384,\n",
       "         -2.5609, -2.7517, -2.7093, -2.6009, -2.6051, -2.4932],\n",
       "        [-2.6727, -2.7465, -2.6631, -2.5796, -2.6171, -2.6319, -2.6800, -2.7043,\n",
       "         -2.5710, -2.7722, -2.6232, -2.5840, -2.5945, -2.5372],\n",
       "        [-2.6771, -2.7824, -2.6728, -2.5547, -2.6193, -2.6391, -2.6817, -2.7014,\n",
       "         -2.5341, -2.7779, -2.6559, -2.5890, -2.5809, -2.5232],\n",
       "        [-2.6507, -2.7859, -2.6643, -2.5588, -2.6027, -2.6128, -2.7102, -2.7192,\n",
       "         -2.5293, -2.7688, -2.6726, -2.6021, -2.5695, -2.5433],\n",
       "        [-2.6766, -2.7000, -2.6013, -2.5960, -2.5884, -2.6869, -2.6436, -2.7390,\n",
       "         -2.5318, -2.7606, -2.6680, -2.6791, -2.5755, -2.5334],\n",
       "        [-2.6564, -2.7700, -2.6414, -2.6182, -2.6136, -2.6476, -2.6881, -2.7039,\n",
       "         -2.5322, -2.7394, -2.6141, -2.6132, -2.6033, -2.5353],\n",
       "        [-2.6933, -2.7848, -2.5806, -2.4062, -2.6815, -2.6036, -2.6745, -2.7830,\n",
       "         -2.5394, -2.7619, -2.7016, -2.6566, -2.6255, -2.5298],\n",
       "        [-2.6503, -2.7405, -2.6085, -2.5650, -2.6202, -2.6334, -2.6601, -2.7275,\n",
       "         -2.5883, -2.7363, -2.6740, -2.6456, -2.5584, -2.5640],\n",
       "        [-2.6855, -2.7876, -2.6258, -2.5737, -2.5836, -2.6087, -2.6762, -2.7689,\n",
       "         -2.5236, -2.7314, -2.6506, -2.6038, -2.6220, -2.5465],\n",
       "        [-2.7136, -2.7268, -2.6415, -2.5461, -2.6563, -2.5449, -2.6601, -2.8788,\n",
       "         -2.6093, -2.6808, -2.5894, -2.6419, -2.5573, -2.5523],\n",
       "        [-2.6833, -2.7673, -2.6589, -2.5620, -2.6053, -2.6415, -2.6721, -2.7087,\n",
       "         -2.5605, -2.7796, -2.6233, -2.5984, -2.5636, -2.5578],\n",
       "        [-2.6647, -2.7555, -2.5877, -2.5836, -2.6429, -2.6383, -2.6559, -2.7420,\n",
       "         -2.5404, -2.7130, -2.6659, -2.6277, -2.6017, -2.5550],\n",
       "        [-2.6581, -2.7614, -2.6335, -2.6070, -2.5883, -2.6214, -2.6855, -2.7127,\n",
       "         -2.5738, -2.7309, -2.6677, -2.5900, -2.5873, -2.5552],\n",
       "        [-2.6588, -2.7468, -2.6833, -2.5774, -2.6116, -2.6409, -2.6732, -2.6756,\n",
       "         -2.5620, -2.7336, -2.6375, -2.6184, -2.6109, -2.5400],\n",
       "        [-2.7242, -2.7156, -2.6271, -2.5202, -2.6339, -2.6669, -2.6513, -2.7396,\n",
       "         -2.5648, -2.7551, -2.6907, -2.5652, -2.6393, -2.4961],\n",
       "        [-2.6603, -2.7959, -2.6691, -2.6167, -2.5728, -2.6103, -2.6430, -2.7125,\n",
       "         -2.5650, -2.7439, -2.6651, -2.6071, -2.6427, -2.4823],\n",
       "        [-2.6869, -2.7446, -2.6725, -2.5805, -2.6017, -2.6254, -2.6625, -2.7001,\n",
       "         -2.5994, -2.7437, -2.6409, -2.6076, -2.5707, -2.5364],\n",
       "        [-2.6609, -2.7408, -2.6529, -2.6046, -2.6043, -2.6293, -2.6611, -2.7188,\n",
       "         -2.5768, -2.7501, -2.6463, -2.6124, -2.5787, -2.5356],\n",
       "        [-2.6770, -2.7176, -2.6235, -2.5581, -2.6269, -2.6435, -2.6768, -2.7576,\n",
       "         -2.5128, -2.7636, -2.6676, -2.6330, -2.5901, -2.5365],\n",
       "        [-2.7617, -2.7849, -2.5682, -2.4784, -2.6231, -2.6589, -2.6482, -2.7096,\n",
       "         -2.5526, -2.8259, -2.6548, -2.5779, -2.6360, -2.5313],\n",
       "        [-2.7229, -2.6888, -2.5502, -2.5480, -2.6847, -2.6809, -2.6511, -2.7562,\n",
       "         -2.5609, -2.7567, -2.6771, -2.6145, -2.6004, -2.4979],\n",
       "        [-2.6894, -2.7745, -2.6736, -2.6063, -2.6037, -2.5531, -2.6867, -2.7663,\n",
       "         -2.4803, -2.7365, -2.7219, -2.6041, -2.5900, -2.5158],\n",
       "        [-2.6999, -2.7377, -2.6912, -2.5542, -2.6034, -2.5955, -2.6543, -2.7505,\n",
       "         -2.6039, -2.7377, -2.6328, -2.5897, -2.5759, -2.5518],\n",
       "        [-2.6781, -2.7411, -2.6582, -2.5737, -2.6073, -2.6116, -2.6976, -2.6957,\n",
       "         -2.5533, -2.7383, -2.6557, -2.5959, -2.6125, -2.5537],\n",
       "        [-2.6462, -2.7462, -2.5850, -2.5720, -2.6440, -2.6765, -2.6864, -2.7493,\n",
       "         -2.5692, -2.7016, -2.6384, -2.5939, -2.6264, -2.5393],\n",
       "        [-2.6566, -2.7341, -2.6269, -2.5945, -2.6359, -2.6085, -2.6426, -2.7447,\n",
       "         -2.5743, -2.7706, -2.6478, -2.6781, -2.5587, -2.5086],\n",
       "        [-2.7345, -2.7581, -2.6162, -2.5249, -2.5959, -2.6210, -2.6648, -2.6760,\n",
       "         -2.5741, -2.8163, -2.6459, -2.5957, -2.5975, -2.5677],\n",
       "        [-2.6613, -2.7310, -2.6921, -2.5991, -2.6212, -2.5863, -2.6921, -2.7388,\n",
       "         -2.5481, -2.7588, -2.6245, -2.6318, -2.5738, -2.5227],\n",
       "        [-2.6695, -2.7531, -2.6799, -2.5728, -2.5797, -2.6045, -2.7057, -2.6868,\n",
       "         -2.5825, -2.7411, -2.6248, -2.5895, -2.6015, -2.5816],\n",
       "        [-2.6222, -2.7686, -2.6622, -2.5957, -2.6103, -2.5968, -2.6562, -2.7353,\n",
       "         -2.5613, -2.7480, -2.6465, -2.6533, -2.5874, -2.5343],\n",
       "        [-2.6909, -2.7015, -2.6077, -2.5778, -2.5678, -2.6439, -2.6575, -2.7808,\n",
       "         -2.5713, -2.8029, -2.6879, -2.6463, -2.5306, -2.5269],\n",
       "        [-2.6912, -2.7682, -2.6079, -2.5242, -2.6103, -2.6391, -2.6358, -2.7972,\n",
       "         -2.5933, -2.6995, -2.6825, -2.5867, -2.6044, -2.5457],\n",
       "        [-2.7369, -2.7339, -2.5136, -2.6970, -2.6265, -2.7180, -2.6116, -2.9274,\n",
       "         -2.5103, -2.6025, -2.7663, -2.6181, -2.5542, -2.4341],\n",
       "        [-2.7335, -2.7881, -2.6095, -2.5236, -2.5876, -2.7499, -2.6056, -2.8153,\n",
       "         -2.5168, -2.8153, -2.6255, -2.4910, -2.6563, -2.5157],\n",
       "        [-2.6911, -2.7691, -2.6035, -2.5614, -2.6816, -2.6757, -2.6390, -2.7604,\n",
       "         -2.5109, -2.7273, -2.6755, -2.6299, -2.5747, -2.4946],\n",
       "        [-2.6518, -2.7664, -2.6996, -2.5821, -2.6031, -2.5924, -2.6480, -2.7617,\n",
       "         -2.5899, -2.7140, -2.6551, -2.6217, -2.5420, -2.5522],\n",
       "        [-2.6898, -2.7765, -2.6400, -2.5833, -2.6325, -2.6126, -2.7218, -2.7047,\n",
       "         -2.5524, -2.7394, -2.6346, -2.6319, -2.5422, -2.5234],\n",
       "        [-2.7186, -2.7643, -2.6470, -2.5658, -2.6144, -2.5734, -2.6247, -2.7222,\n",
       "         -2.5694, -2.7990, -2.6609, -2.6047, -2.5541, -2.5691]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X_Train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,trainv, log_interval=100):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        #data = data.to(device)\n",
    "        #target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data.type(\"torch.FloatTensor\"))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.data.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "    trainv.append( train_loss / float(len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        #data = data.to(device)\n",
    "        #target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-bbf87aa5c56d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, trainv, log_interval)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 2\n",
    "\n",
    "lossv, accv, trainv = [], [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, trainv)\n",
    "    #validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".  [See more here on how to load the model back](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) if you want to continue training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You should return your predictions for the test set in a plain text file.  The text file contains one row for each test set image.  Each row contains a binary prediction for each label (separated by a single space), 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "The order of the rows should be according to the numeric order of the image numbers.  In the test set, this means that the first row refers to image `im20001.jpg`, the second to `im20002.jpg`, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the prediction output matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
